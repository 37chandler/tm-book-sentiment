{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Plots\n",
    "In this notebook we reprise an [analysis](https://infogram.com/blog/scientists-use-big-data-to-discover-6-basic-emotional-story-arcs/) discussed in _Everybody Lies_. In this chapter Stephens-Davidowitz details an approach to measuring plots, looking at sentiment across a book. He posits six main plots:\n",
    "\n",
    "1. Rags to Riches (rise)\n",
    "1. Riches to Rags (fall)\n",
    "1. Person in a hole (fall-rise)\n",
    "1. Icarus (rise-fall)\n",
    "1. Cinderella (rise-fall-rise)\n",
    "1. Oedipus (fall-rise-fall)\n",
    "\n",
    "In this notebook we'll pick a book from NLTK and do this. (Note that NLTK, by providing digitized copies of many books, has saved us a _lot_ of effort here.) \n",
    "\n",
    "## Getting more books\n",
    "There are about 30 books available from [Project Gutenberg](https://www.gutenberg.org/) via NLTK. There are *many* more available, though. Those books are accessible from a Python package [`gutenberg`](https://pypi.python.org/pypi/Gutenberg). Unfortunately, that package is dependent on an Oracle DB package (BSD-DB), which is a hassle to load. For now, let's roll with NLTK.  \n",
    "\n",
    "## Results from the Paper\n",
    "Here's a graph from the technical results of the analysis:\n",
    "![six story lines](https://about.infogr.am/wp-content/uploads/2016/10/storyarcs-1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by looking at the books that came along with our NLTK installation\n",
    "print(nltk.corpus.gutenberg.fileids())\n",
    "print(nltk.corpus.shakespeare.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick one of these and assign it a shorter name.\n",
    "book = nltk.corpus.gutenberg.words(\"whitman-leaves.txt\") # using the \"words\" function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what we've got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(book))\n",
    "print(book[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do sentiment analysis, we're going to need a list of words that indicate positivity or negativity. NLTK has a bunch of this functionality built in. Let's start by playing around with the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your own text here\n",
    "my_text = \"Here's a cool sentence. If I'm being honest. Or just saying it.\"\n",
    "demo_liu_hu_lexicon(my_text,plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `demo_liu_hu_lexicon` function does sentiment classification based on the [Liu and Hu opinion lexicon](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html). The plotting is optional, but it tells you how the algorithm is working. \n",
    "\n",
    "Another useful set of lexicons comes with `tidytext`, a pretty sweet [book](http://tidytextmining.com/) detailing how to do text mining in R. I've downloaded their sentiment file and put it a text file called `tidytext_sentiments.txt`. This file is tab delimited, with headers and had three columns. The word, the sentiment (\"positive\" or \"negative\"), and the lexicon source of the word. \n",
    "\n",
    "In the cell below, read this file in and create a dictionary. The words will be the key and the value of the dictionary will be `1` if the word is positive and `-1` if the word is negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_scores = {}\n",
    "            \n",
    "# Open the file `tidytext_sentiments.txt`\n",
    "# Fill up sentiment scores so it has values like \n",
    "# sentiment_scores['awesome'] = 1\n",
    "\n",
    "assert(len(sentiment_scores) > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, let's take a look at the data to see if it looks correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, word in enumerate(sentiment_scores) :\n",
    "    print(\"{} has score {}.\".format(word,sentiment_scores[word]))\n",
    "    if idx > 30 :\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do some sentiment scoring. Your book is laid out as a series of words, thanks to the `.words()` function in NLTK. \n",
    "\n",
    "Think about what a reasonable algorithm for measuring book sentiment would be. Then you'll implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a place for you to create your running score through your book.\n",
    "# Your score vector should be the same length as the book. When you \n",
    "# hit a positive word, the score should go up by one. When you hit a\n",
    "# negative word, the score should go down by one. So this score\n",
    "# vector holds a cumulative sum across the book. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll write out our sentiment score to a text file, so that we can plot it in `ggplot` in R (only for those in ADA; I've included a file `plot_scores.r` to do this for you). Write this out as two columns, one being the index and the other being the score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's a place to write out your file. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
